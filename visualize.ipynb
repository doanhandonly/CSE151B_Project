{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "from skimage import io\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder 'A' at path: ./archive/Train_Alphabet/A\n",
      "Found 900 images in ./archive/Train_Alphabet/A\n",
      "Loading folder 'B' at path: ./archive/Train_Alphabet/B\n",
      "Found 900 images in ./archive/Train_Alphabet/B\n",
      "Loading folder 'Blank' at path: ./archive/Train_Alphabet/Blank\n",
      "Found 900 images in ./archive/Train_Alphabet/Blank\n",
      "Loading folder 'C' at path: ./archive/Train_Alphabet/C\n",
      "Found 900 images in ./archive/Train_Alphabet/C\n",
      "Loading folder 'D' at path: ./archive/Train_Alphabet/D\n",
      "Found 900 images in ./archive/Train_Alphabet/D\n",
      "Loading folder 'E' at path: ./archive/Train_Alphabet/E\n",
      "Found 900 images in ./archive/Train_Alphabet/E\n",
      "Loading folder 'F' at path: ./archive/Train_Alphabet/F\n",
      "Found 900 images in ./archive/Train_Alphabet/F\n",
      "Loading folder 'G' at path: ./archive/Train_Alphabet/G\n",
      "Found 900 images in ./archive/Train_Alphabet/G\n",
      "Loading folder 'H' at path: ./archive/Train_Alphabet/H\n",
      "Found 901 images in ./archive/Train_Alphabet/H\n",
      "Loading folder 'I' at path: ./archive/Train_Alphabet/I\n",
      "Found 900 images in ./archive/Train_Alphabet/I\n",
      "Loading folder 'J' at path: ./archive/Train_Alphabet/J\n",
      "Found 900 images in ./archive/Train_Alphabet/J\n",
      "Loading folder 'K' at path: ./archive/Train_Alphabet/K\n",
      "Found 900 images in ./archive/Train_Alphabet/K\n",
      "Loading folder 'L' at path: ./archive/Train_Alphabet/L\n",
      "Found 900 images in ./archive/Train_Alphabet/L\n",
      "Loading folder 'M' at path: ./archive/Train_Alphabet/M\n",
      "Found 900 images in ./archive/Train_Alphabet/M\n",
      "Loading folder 'N' at path: ./archive/Train_Alphabet/N\n",
      "Found 900 images in ./archive/Train_Alphabet/N\n",
      "Loading folder 'O' at path: ./archive/Train_Alphabet/O\n",
      "Found 900 images in ./archive/Train_Alphabet/O\n",
      "Loading folder 'P' at path: ./archive/Train_Alphabet/P\n",
      "Found 900 images in ./archive/Train_Alphabet/P\n",
      "Loading folder 'Q' at path: ./archive/Train_Alphabet/Q\n",
      "Found 900 images in ./archive/Train_Alphabet/Q\n",
      "Loading folder 'R' at path: ./archive/Train_Alphabet/R\n",
      "Found 900 images in ./archive/Train_Alphabet/R\n",
      "Loading folder 'S' at path: ./archive/Train_Alphabet/S\n",
      "Found 900 images in ./archive/Train_Alphabet/S\n",
      "Loading folder 'T' at path: ./archive/Train_Alphabet/T\n",
      "Found 900 images in ./archive/Train_Alphabet/T\n",
      "Loading folder 'U' at path: ./archive/Train_Alphabet/U\n",
      "Found 900 images in ./archive/Train_Alphabet/U\n",
      "Loading folder 'V' at path: ./archive/Train_Alphabet/V\n",
      "Found 900 images in ./archive/Train_Alphabet/V\n",
      "Loading folder 'W' at path: ./archive/Train_Alphabet/W\n",
      "Found 900 images in ./archive/Train_Alphabet/W\n",
      "Loading folder 'X' at path: ./archive/Train_Alphabet/X\n",
      "Found 900 images in ./archive/Train_Alphabet/X\n",
      "Loading folder 'Y' at path: ./archive/Train_Alphabet/Y\n",
      "Found 900 images in ./archive/Train_Alphabet/Y\n",
      "Loading folder 'Z' at path: ./archive/Train_Alphabet/Z\n",
      "Found 900 images in ./archive/Train_Alphabet/Z\n",
      "Total images loaded: 24301\n",
      "Training images shape: (24301, 100, 120)\n",
      "Training labels shape: (24301,)\n",
      "Training label map: {'A': 0, 'B': 1, 'Blank': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n",
      "Loading folder 'A' at path: ./archive/Test_Alphabet/A\n",
      "Found 100 images in ./archive/Test_Alphabet/A\n",
      "Loading folder 'B' at path: ./archive/Test_Alphabet/B\n",
      "Found 100 images in ./archive/Test_Alphabet/B\n",
      "Loading folder 'Blank' at path: ./archive/Test_Alphabet/Blank\n",
      "Found 100 images in ./archive/Test_Alphabet/Blank\n",
      "Loading folder 'C' at path: ./archive/Test_Alphabet/C\n",
      "Found 100 images in ./archive/Test_Alphabet/C\n",
      "Loading folder 'D' at path: ./archive/Test_Alphabet/D\n",
      "Found 100 images in ./archive/Test_Alphabet/D\n",
      "Loading folder 'E' at path: ./archive/Test_Alphabet/E\n",
      "Found 100 images in ./archive/Test_Alphabet/E\n",
      "Loading folder 'F' at path: ./archive/Test_Alphabet/F\n",
      "Found 100 images in ./archive/Test_Alphabet/F\n",
      "Loading folder 'G' at path: ./archive/Test_Alphabet/G\n",
      "Found 100 images in ./archive/Test_Alphabet/G\n",
      "Loading folder 'H' at path: ./archive/Test_Alphabet/H\n",
      "Found 100 images in ./archive/Test_Alphabet/H\n",
      "Loading folder 'I' at path: ./archive/Test_Alphabet/I\n",
      "Found 100 images in ./archive/Test_Alphabet/I\n",
      "Loading folder 'J' at path: ./archive/Test_Alphabet/J\n",
      "Found 100 images in ./archive/Test_Alphabet/J\n",
      "Loading folder 'K' at path: ./archive/Test_Alphabet/K\n",
      "Found 100 images in ./archive/Test_Alphabet/K\n",
      "Loading folder 'L' at path: ./archive/Test_Alphabet/L\n",
      "Found 100 images in ./archive/Test_Alphabet/L\n",
      "Loading folder 'M' at path: ./archive/Test_Alphabet/M\n",
      "Found 100 images in ./archive/Test_Alphabet/M\n",
      "Loading folder 'N' at path: ./archive/Test_Alphabet/N\n",
      "Found 100 images in ./archive/Test_Alphabet/N\n",
      "Loading folder 'O' at path: ./archive/Test_Alphabet/O\n",
      "Found 100 images in ./archive/Test_Alphabet/O\n",
      "Loading folder 'P' at path: ./archive/Test_Alphabet/P\n",
      "Found 100 images in ./archive/Test_Alphabet/P\n",
      "Loading folder 'Q' at path: ./archive/Test_Alphabet/Q\n",
      "Found 100 images in ./archive/Test_Alphabet/Q\n",
      "Loading folder 'R' at path: ./archive/Test_Alphabet/R\n",
      "Found 100 images in ./archive/Test_Alphabet/R\n",
      "Loading folder 'S' at path: ./archive/Test_Alphabet/S\n",
      "Found 100 images in ./archive/Test_Alphabet/S\n",
      "Loading folder 'T' at path: ./archive/Test_Alphabet/T\n",
      "Found 100 images in ./archive/Test_Alphabet/T\n",
      "Loading folder 'U' at path: ./archive/Test_Alphabet/U\n",
      "Found 100 images in ./archive/Test_Alphabet/U\n",
      "Loading folder 'V' at path: ./archive/Test_Alphabet/V\n",
      "Found 100 images in ./archive/Test_Alphabet/V\n",
      "Loading folder 'W' at path: ./archive/Test_Alphabet/W\n",
      "Found 100 images in ./archive/Test_Alphabet/W\n",
      "Loading folder 'X' at path: ./archive/Test_Alphabet/X\n",
      "Found 100 images in ./archive/Test_Alphabet/X\n",
      "Loading folder 'Y' at path: ./archive/Test_Alphabet/Y\n",
      "Found 100 images in ./archive/Test_Alphabet/Y\n",
      "Loading folder 'Z' at path: ./archive/Test_Alphabet/Z\n",
      "Found 100 images in ./archive/Test_Alphabet/Z\n",
      "Total images loaded: 2700\n",
      "Testing images shape: (2700, 100, 120)\n",
      "Testing labels shape: (2700,)\n",
      "Testing label map: {'A': 0, 'B': 1, 'Blank': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegowilliams/Downloads/CSE151B_Project_ CamSeg/asl_venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-03-20 22:37:21.575288: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-03-20 22:37:21.575456: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-03-20 22:37:21.575702: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-03-20 22:37:21.576920: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-20 22:37:21.577587: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41216</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,275,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,483</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m57\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m57\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41216\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,275,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)             │         \u001b[38;5;34m3,483\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,298,459</span> (20.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,298,459\u001b[0m (20.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,298,267</span> (20.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,298,267\u001b[0m (20.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 22:37:27.227395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 172ms/step - accuracy: 0.0685 - loss: 106.2156 - val_accuracy: 0.1307 - val_loss: 13.4340\n",
      "Epoch 2/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 211ms/step - accuracy: 0.4343 - loss: 29.4618 - val_accuracy: 0.8281 - val_loss: 2.1578\n",
      "Epoch 3/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 248ms/step - accuracy: 0.7596 - loss: 7.6610 - val_accuracy: 0.9426 - val_loss: 0.8394\n",
      "Epoch 4/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 183ms/step - accuracy: 0.8564 - loss: 3.7872 - val_accuracy: 0.9304 - val_loss: 1.4691\n",
      "Epoch 5/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 197ms/step - accuracy: 0.9113 - loss: 2.0532 - val_accuracy: 0.9626 - val_loss: 0.6474\n",
      "Epoch 6/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 194ms/step - accuracy: 0.9255 - loss: 1.6193 - val_accuracy: 0.8741 - val_loss: 5.4896\n",
      "Epoch 7/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 256ms/step - accuracy: 0.9357 - loss: 1.2858 - val_accuracy: 0.9741 - val_loss: 0.4909\n",
      "Epoch 8/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 180ms/step - accuracy: 0.9470 - loss: 0.9778 - val_accuracy: 0.9652 - val_loss: 0.7911\n",
      "Epoch 9/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 191ms/step - accuracy: 0.9461 - loss: 1.1381 - val_accuracy: 0.8611 - val_loss: 3.4185\n",
      "Epoch 10/10\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 192ms/step - accuracy: 0.9495 - loss: 1.1123 - val_accuracy: 0.9733 - val_loss: 0.5260\n",
      "Model training complete. Saved as 'asl_gesture_recognition.keras'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_PATH = \"./archive\"\n",
    "\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 100, 120\n",
    "NUM_CLASSES = 27\n",
    "\n",
    "def load_dataset(data_folder, img_width=120, img_height=100):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_counter = 0\n",
    "\n",
    "    for folder in sorted(os.listdir(data_folder)):\n",
    "        folder_path = os.path.join(data_folder, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading folder '{folder}' at path: {folder_path}\")\n",
    "        \n",
    "        if folder not in label_map:\n",
    "            label_map[folder] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        exts = (\"*.jpg\", \"*.JPG\", \"*.jpeg\", \"*.png\")\n",
    "        image_files = []\n",
    "        for ext in exts:\n",
    "            image_files.extend(glob.glob(os.path.join(folder_path, ext)))\n",
    "        \n",
    "        print(\"Found\", len(image_files), \"images in\", folder_path)\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            images.append(img)\n",
    "            labels.append(label_map[folder])\n",
    "    \n",
    "    print(\"Total images loaded:\", len(images))\n",
    "    return np.array(images), np.array(labels), label_map\n",
    "\n",
    "DATASET_PATH = \"./archive\"\n",
    "\n",
    "# --------------------\n",
    "# Load training images\n",
    "# --------------------\n",
    "train_folder = os.path.join(DATASET_PATH, \"Train_Alphabet\")\n",
    "X_train, y_train, train_label_map = load_dataset(train_folder)\n",
    "print(\"Training images shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Training label map:\", train_label_map)\n",
    "\n",
    "# ------------------\n",
    "# Load testing images\n",
    "# ------------------\n",
    "test_folder = os.path.join(DATASET_PATH, \"Test_Alphabet\")\n",
    "X_test, y_test, test_label_map = load_dataset(test_folder)\n",
    "print(\"Testing images shape:\", X_test.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)\n",
    "print(\"Testing label map:\", test_label_map)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "X_test = X_test.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"asl_gesture_recognition.keras\")\n",
    "\n",
    "print(\"Model training complete. Saved as 'asl_gesture_recognition.keras'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 22:51:35.705 Python[1469:23803] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-20 22:51:35.705 Python[1469:23803] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m top, right, bottom, left \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m600\u001b[39m  \n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     grabbed, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m grabbed:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] Frame not grabbed. Skipping iteration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "def _load_weights():\n",
    "    try:\n",
    "        model = load_model(\"asl_gesture_recognition.keras\")\n",
    "        print(\"[INFO] Model loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def segment_hand(image):\n",
    "    \"\"\"\n",
    "    Segments the hand from the background using adaptive thresholding.\n",
    "    Returns the thresholded image and the largest contour (hand).\n",
    "    \"\"\"\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "    thresholded = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                        cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return None, None\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    return thresholded, largest_contour\n",
    "\n",
    "def getPredictedClass(model):\n",
    "    image = cv2.imread('Temp.png')\n",
    "    if image is None:\n",
    "        print(\"[ERROR] Temp.png not found.\")\n",
    "        return \"Unknown\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.resize(gray_image, (100, 120))\n",
    "\n",
    "    gray_image = gray_image / 255.0\n",
    "    gray_image = gray_image.reshape(1, 100, 120, 1)\n",
    "\n",
    "    prediction = model.predict_on_batch(gray_image)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return chr(65 + predicted_class) if predicted_class < 26 else \"Unknown\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    if not camera.isOpened():\n",
    "        print(\"[ERROR] Camera could not be opened.\")\n",
    "        exit()\n",
    "\n",
    "    model = _load_weights()\n",
    "    if model is None:\n",
    "        exit() \n",
    "\n",
    "    top, right, bottom, left = 100, 300, 400, 600  \n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = camera.read()\n",
    "        if not grabbed:\n",
    "            print(\"[ERROR] Frame not grabbed. Skipping iteration...\")\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        clone = frame.copy()\n",
    "\n",
    "        roi = frame[top:bottom, right:left]\n",
    "        thresholded, hand_contour = segment_hand(roi)\n",
    "\n",
    "        if thresholded is not None:\n",
    "            cv2.drawContours(clone, [hand_contour + (right, top)], -1, (0, 0, 255), 2)\n",
    "            cv2.imwrite('Temp.png', thresholded)\n",
    "            predictedClass = getPredictedClass(model)\n",
    "            cv2.putText(clone, predictedClass, (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"Thresholded\", thresholded)\n",
    "        cv2.rectangle(clone, (right, top), (left, bottom), (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "        \n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
